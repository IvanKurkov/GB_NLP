{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "9fe5826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "9019faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = []\n",
    "with open('sh.txt', 'r', encoding='utf-8') as outfile:\n",
    "    for i in outfile.readlines():\n",
    "        if i != '\\n':\n",
    "            i = i.rstrip()\n",
    "            sh.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "32e82289",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sh = pd.DataFrame(sh, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "154956f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sh['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "f417f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushkin = []\n",
    "with open('Pushkin.txt', 'r', encoding='utf-8') as outfile:\n",
    "    for i in outfile.readlines():\n",
    "        if i != '\\n':\n",
    "            i = i.strip()\n",
    "            pushkin.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "0120f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushkin = pushkin[:245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "4a06fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pushkin = pd.DataFrame(pushkin, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "3ef4f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pushkin['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "f0bf3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih=[]\n",
    "with open('stih.txt', 'r', encoding='utf-8') as outfile:\n",
    "    for i in outfile.readlines():\n",
    "        if i != '\\n':\n",
    "            i = i.strip()\n",
    "            stih.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "1ab9cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtext(text):\n",
    "    text = f'{random.choice(stih)} {text}'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "e785ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pushkin['text'] = data_pushkin['text'].apply(addtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "e5cfc22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12316\\3648041625.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df= data_pushkin.append(data_sh, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df= data_pushkin.append(data_sh, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "14bbd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90497b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "f4ac80b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "bert = TFAutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "8357a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "7fee453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df['class'].values  # take sentiment column in df as array\n",
    "labels = np.zeros((arr.size, arr.max()+1))  # initialize empty (all zero) label array\n",
    "labels[np.arange(arr.size), arr] = 1  # add ones in indices where we have a value\n",
    "\n",
    "# define function to handle tokenization\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=30,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "# initialize two arrays for input tensors\n",
    "Xids = np.zeros((len(df), SEQ_LEN))\n",
    "Xmask = np.zeros((len(df), SEQ_LEN))\n",
    "\n",
    "# loop through data and tokenize everything\n",
    "for i, sentence in enumerate(df['text']):\n",
    "    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n",
    "\n",
    "# create tensorflow dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "# restructure dataset format for BERT\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "  \n",
    "dataset = dataset.map(map_func)  # apply the mapping function\n",
    "\n",
    "# shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(10000).batch(32)\n",
    "\n",
    "DS_LEN = len(list(dataset))  # get dataset length\n",
    "\n",
    "SPLIT = 0.9  # we will create a 90-10 split\n",
    "\n",
    "# create training-validation sets\n",
    "train = dataset.take(round(DS_LEN*SPLIT))\n",
    "val = dataset.skip(round(DS_LEN*SPLIT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab31dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "873e04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "input_ids = tf.keras.layers.Input(shape=(30,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(30,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "X = tf.keras.layers.GRU(128)(embeddings)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.7)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='sigmoid',  name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# freeze the BERT layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "28e838ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 13s 66ms/step - loss: 0.7693 - accuracy: 0.6612\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.5011 - accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.3835 - accuracy: 0.8673\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.3173 - accuracy: 0.8857\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.2590 - accuracy: 0.9184\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.2266 - accuracy: 0.9143\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1616 - accuracy: 0.9510\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1457 - accuracy: 0.9429\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1493 - accuracy: 0.9531\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1227 - accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "e319276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids, Xmask = tokenize(\"Привет, твои как дела?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "f85f95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'input_ids': Xids, 'attention_mask': Xmask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "fa713b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.4714664 , 0.43188226]], dtype=float32)>"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "4df9affa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "c11e28ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.1289255 , 0.81518775]], dtype=float32)>"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids, Xmask = tokenize(\"Что сказал Путин?\")\n",
    "test = {'input_ids': Xids, 'attention_mask': Xmask}\n",
    "model(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "1459e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "ab84892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4004165"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(model(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "66fe4ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.9723932 , 0.01688579]], dtype=float32)>"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids, Xmask = tokenize(\"Помоги продолжить стих: Пойдем покатаемся\")\n",
    "test = {'input_ids': Xids, 'attention_mask': Xmask}\n",
    "model(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "a293581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/first_classif.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "7f366961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "first_classif_model = tf.keras.models.load_model('models/first_classif.h5', custom_objects={\"TFBertModel\": transformers.TFBertModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "06500a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.9330265, 0.030246 ]], dtype=float32)>"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids, Xmask = tokenize(\"Помоги сочинить стих: Природа любит\")\n",
    "test = {'input_ids': Xids, 'attention_mask': Xmask}\n",
    "first_classif_model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454e0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013c0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
