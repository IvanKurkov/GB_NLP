{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "68337491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4ca11117",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = []\n",
    "with open('sh.txt', 'r', encoding='utf-8') as outfile:\n",
    "    for i in outfile.readlines():\n",
    "        if i != '\\n':\n",
    "            i = i.rstrip()\n",
    "            sh.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "b7fc2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sh = pd.DataFrame(sh, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a8d4c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sh['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "039b5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushkin = []\n",
    "with open('Pushkin.txt', 'r', encoding='utf-8') as outfile:\n",
    "    for i in outfile.readlines():\n",
    "        if i != '\\n':\n",
    "            i = i.strip()\n",
    "            pushkin.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c6bb8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushkin = pushkin[:245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7cb06af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pushkin = pd.DataFrame(pushkin, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "878f22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pushkin['class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "273ef77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih=[]\n",
    "with open('stih.txt', 'r', encoding='utf-8') as outfile:\n",
    "    for i in outfile.readlines():\n",
    "        if i != '\\n':\n",
    "            i = i.strip()\n",
    "            stih.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "57998ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtext(text):\n",
    "    text = f'{random.choice(stih)} {text}'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "afbd4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pushkin['text'] = data_pushkin['text'].apply(addtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e2a50895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12316\\3648041625.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df= data_pushkin.append(data_sh, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df= data_pushkin.append(data_sh, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "7b77d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6d1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "56c76eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "bert = TFAutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d84c48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "25809d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df['class'].values  # take sentiment column in df as array\n",
    "labels = np.zeros((arr.size, arr.max()+1))  # initialize empty (all zero) label array\n",
    "labels[np.arange(arr.size), arr] = 1  # add ones in indices where we have a value\n",
    "\n",
    "# define function to handle tokenization\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=20,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "# initialize two arrays for input tensors\n",
    "Xids = np.zeros((len(df), SEQ_LEN))\n",
    "Xmask = np.zeros((len(df), SEQ_LEN))\n",
    "\n",
    "# loop through data and tokenize everything\n",
    "for i, sentence in enumerate(df['text']):\n",
    "    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n",
    "\n",
    "# create tensorflow dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "# restructure dataset format for BERT\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "  \n",
    "dataset = dataset.map(map_func)  # apply the mapping function\n",
    "\n",
    "# shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(10000).batch(32)\n",
    "\n",
    "DS_LEN = len(list(dataset))  # get dataset length\n",
    "\n",
    "SPLIT = 0.9  # we will create a 90-10 split\n",
    "\n",
    "# create training-validation sets\n",
    "train = dataset.take(round(DS_LEN*SPLIT))\n",
    "val = dataset.skip(round(DS_LEN*SPLIT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f97b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "335bb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "input_ids = tf.keras.layers.Input(shape=(20,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(20,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "X = tf.keras.layers.GRU(256)(embeddings)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='sigmoid',  name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# freeze the BERT layer\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4d042db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0959 - accuracy: 0.9735\n",
      "Epoch 2/3\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0521 - accuracy: 0.9878\n",
      "Epoch 3/3\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0230 - accuracy: 0.9959\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "01f15c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids, Xmask = tokenize(\"Привет, как дела?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0da6aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'input_ids': Xids, 'attention_mask': Xmask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "0e2167ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.68082863, 0.29687938]], dtype=float32)>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7e9c3c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "2abfeb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.2229206, 0.7144924]], dtype=float32)>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids, Xmask = tokenize(\"Кто взорвал Северный Поток?\")\n",
    "test = {'input_ids': Xids, 'attention_mask': Xmask}\n",
    "model(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a0c6bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "1d91e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7144924"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(model(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6d8bd5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.8112332 , 0.16258779]], dtype=float32)>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids, Xmask = tokenize(\"Помоги продолжить фразу: Пойдем покатаемся\")\n",
    "test = {'input_ids': Xids, 'attention_mask': Xmask}\n",
    "model(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9150bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/first_classif.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "14195c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "first_classif_model = tf.keras.models.load_model('models/first_classif.h5', custom_objects={\"TFBertModel\": transformers.TFBertModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "0e332273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.9874334 , 0.02035673]], dtype=float32)>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids, Xmask = tokenize(\"Помоги сочинить стих: Природа любит\")\n",
    "test = {'input_ids': Xids, 'attention_mask': Xmask}\n",
    "first_classif_model(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
